{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Take-Home Assignment - Data Analysis and Quantitative Trading (2023-24) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is an individual assignment. It has been solved by:** \n",
    "\n",
    "|     Name       | Student number    | Email           |\n",
    "| :------------: | :---------------: | :-------------: | \n",
    "| [name] |       [student number]    |    [email]      |\n",
    "\n",
    "Please fill your credentials in the table above by double clicking on the text and replacing the current text in brackets by your own name, student number and email. \n",
    "\n",
    "By submitting this assignment you consent to the University's rules regarding plagiarism and cheating. In particular, sharing code with any students outside your group is strictly prohibited. AI tools such as ChatGPT can be used to obtain reference information (e.g., you can ask ChatGPT questions about how a command works). However you are forbidden from entering any of the assignment questions as a prompt into an AI tool, or submitted text generated by an AI tool as part of the assignment solution. Any suspected instances will be reported to the Board of Examiners.\n",
    "\n",
    "In addition to submitting the filled-out Jupyter Notebook and output files, each student must submit a recording that explains the coding process. In the recording, you should first explain in your own words how you plan to execute certain substeps, without referring to the code. Then, you should review the key steps of the code (you do not need to review basic commands that we have applied frequently throughout the class). Students should review the following substeps in the recordings:\n",
    "- Students with an SIS ID that ends in an odd number should review **Steps 2(d), 2(e), 2(j)**, and **3(d)**. \n",
    "- Students with an SIS ID that ends in an odd number should review **Steps 2(f), 2(g), 2(j)**, and **3(c)**. \n",
    "\n",
    "If you do not manage to finish all of the subsets, then review the steps that you did complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Deciding how to Close out the Basis Trade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** The CDS-Bond Basis formula establishes the true CDS spread, relative to the coupon rate of a 5-year corporate bond trading at par value. Quantitative trading funds can compare this true spread to the market CDS spread, to decide whether to engage in an arbitrage trade. While this decision is relatively straightforward, the decision of when and how to close the arbitrage trade is more complicated, with no clear theoretical guidance.\n",
    "\n",
    "In this step, you will demonstrate your understanding of the process and challenges faced when closing out a CDS-Bond Basis arbitrage trade. No programming is required for this step. \n",
    "\n",
    "**Getting Started**: To complete this part, you should review the slides for lectures 2 and 3. You do not need any data files.\n",
    "\n",
    "**To Complete:**\n",
    "- Fill in all answer boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1(a)**     \n",
    "Consider the example on lecture slides 15 and 17, which uses a simple version of the CDS-Bond Basis with market CDS spread of 3.25%. Suppose that a 5-year bond and 5-year CDS (described on slide 15) are both issued on April 1, 2022. On May 1, the CDS spread is 3.25% and you open an arbitrage trade. On May 15 the CDS spread rises to 3.57%. \n",
    "\n",
    "Suppose you choose to close out the basis trade on May 15. Explain the process and the total profit that you would receive. (Assume that spreads on all CDS contracts must be paid annually, at the end of each year that the CDS is outstanding. Thus an investor who buys a CDS on January 1, 2022 must make the first spread payment on December 31, 2022, and an investor who writes the same CDS receives the first spread payment on that date.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Click on this box and type your answer. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1(b)**     \n",
    "A researcher within your fund documents that CDS spreads display significant momentum---when CDS spreads rise for two weeks, then on average they tend to continue rising at a similar rate for the next two weeks. Explain the potential benefit of keeping the arbitrage trade from **Step 1(a)** open, as well as the potential risks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Click on this box and type your answer. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1(c)**     \n",
    "Suppose that keeping the trade open in **Step 1(b)** requires the fund to place \\\\$100 of its own cash into a margin account. The cash earns a 0\\% rate of return while the trade is open. Consider two possibilities for what the fund can do with its \\\\$100 cash, if it does not place it in the margin account:\n",
    "- On May 15, your fund identifies another arbitrage opportunity, that over the next two weeks should generate a return of \\\\$300 for each \\\\$100 of own funds invested. The fund cannot invest in this trade if it place \\\\$100 cash in the margin account.\n",
    "- On May 15, the fund does not have any other arbitrage opportunity available. Its best available option is to buy risk-free U.S. Treasury bonds, which would earn interest of \\\\$0.5 for each \\\\$100 of own funds invested.\n",
    "\n",
    "Explain how the different scenarios affect the fund's decision of whether to close the CDS-Bond Basis arbitrage trade or keep it open for two weeks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Click on this box and type your answer. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1(d)**     \n",
    "Re-consider the example from **Step 1(a)**. Suppose that the market CDS spread ranges from 3.2% to 3.4% throughout the year, but then rises to 3.7% on May 15, 2023. If you close the trade using another 5-year CDS contract, what risk is your fund exposed to? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Click on this box and type your answer. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Testing Whether Intangible Assets are Mispriced by the Market"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** A common way for quantitative trading firms to identify mispriced stocks is to first propose a variable that theoretically may be related to stock mispricing, then sort firms into portfolios based on levels of that variable, and then examine whether the portfolios historically earned higher returns than expected (based on a reasonable benchmark such as the Fama-French 3-Factor model).  \n",
    "\n",
    "In this step, you will test whether firms with high amounts of intangible assets are mispriced by investors. Intangibles are non-physical assets that create value for the firm. Prominent examples include software, patents, recognizable brands, a strong organizational culture, and data-driven logistics. Academic research shows that intangibles have significantly grown in value over time. However, financial statements largely do not recognize the value of intangibles, in part because they are much harder to measure than tangible assets. Thus, it is feasible that investors do not correctly calculate the price of firms that rely heavily on intangibles.\n",
    "\n",
    "**Getting Started**: To complete this part, you will need these files:\n",
    "- intangible_capital.txt, which contains annual data on two different types of intangible assets. It covers the years 1970--2019.\n",
    "- compustat_data_final.txt, which is similar to the previous Compustat dataset, but it now contains each firm's stock price and covers the years 1980--2019.\n",
    "- crsp_data_final.txt, which is similar to the previous CRSP dataset but now covers 1980--2019. \n",
    "- factor_data.txt, which is the same file as used previously.\n",
    "\n",
    "**To Complete:**\n",
    "- Fill in all code boxes\n",
    "- Fill in all answer boxes and tables below.\n",
    "- Submit the output files created in **Step 2(j)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2(a)**     \n",
    "Read only the introduction to the paper \"The History of the Cross-Section of Stock\n",
    "Returns\" by Juhani Linnainmaa and Michael Roberts, which was published in 2018 in the *Review of Financial Studies* volume 7. Briefly describe what the paper finds about most stock price anomalies identified by academic research (an anomaly is a potential examples of stock mispricing). Also, describe whether the paper finds any evidence that intangible assets may be mispriced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Click on this box and type your interpretation of the table. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2(b)**     \n",
    "Load intangible_capital.txt into a data frame. Also load compustat_data_final.txt into a separate date frame. Remove two sets of firms from this data frame: 1) Financial and utilities firms; 2) Firms that are incorporated outside the United States. (**NOTE:** We have not previously used a variable for country of incorporation, so you need to browse through compustat_data_final.txt to find the variable. Pandas Notebook Lecture 3 contains some commands for browsing data frames.)\n",
    "\n",
    "Next, merge the two data frames. Choose whether to keep observations that are in both data frames OR to keep observations that are in both data frames and also observations that are in only in intangible_capital.txt. Explain your choice briefly in a comment in your code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## ENTER YOUR CODE IN THIS BOX\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load intangible_capital.txt into a data frame\n",
    "intangible_data = pd.read_csv(\"D:/Quantitative--Trading/Data/intangible_capital.txt\", sep='\\t')\n",
    "# Load compustat_data_final.txt into a separate data frame\n",
    "compustat_data = pd.read_csv('D:/Quantitative--Trading/Data/compustat_data_final.txt', sep='\\t')\n",
    "\n",
    "# Remove financial and utilities firms\n",
    "compustat_data = compustat_data[~compustat_data['sic_industry_code'].isin(['FINANCIAL', 'UTILITY'])]\n",
    "\n",
    "# Remove firms incorporated outside the United States (assuming 'fic' is the variable)\n",
    "compustat_data = compustat_data[compustat_data['fic'] == 'USA']\n",
    "\n",
    "# Merge the two data frames using an inner merge\n",
    "merged_data = pd.merge(compustat_data, intangible_data, on=['gvkey', 'datadate'])\n",
    "\n",
    "# Output the merged data frame\n",
    "merged_data.to_csv('step_2b.txt', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intangible_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2(c)**     \n",
    "A commonly used measure of intangible assets is the ratio (knowledge_capital + organizational_capital)/total_assets. Calculate this ratio for each observation in the sample. Also calculate the market capitalization for each observation.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'knowledge_capital', 'organizational_capital', and 'total_assets' are the relevant columns\n",
    "merged_data['intangible_ratio'] = (merged_data['knowledge_capital'] + merged_data['organizational_capital']) / merged_data['total_assets']\n",
    "\n",
    "# Calculate market capitalization\n",
    "merged_data['market_cap'] = merged_data['stock_price'] * merged_data['shares_outstanding']\n",
    "\n",
    "# Output the updated merged data frame\n",
    "merged_data.to_csv('step_2c.txt', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2(d)**    \n",
    "Create a new 'year' column in the data frame, by extracting the year from the 'datadate' column. Note that this value represents the calendar year in which the firm reported its financial information.  \n",
    "\n",
    "Next, for each year, calculate the quintiles of the intangibles ratio (i.e., the 20$^{th}$, 40$^{th}$, 60$^{th}$, and 80$^{th}$ percentiles of the intangibles ratio in each year). Separately for each year, calculate the quintiles of market capitalization. Create a separate data frame that contains nine columns: The year and all of the quintiles that you have just calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract the year from the 'datadate' column\n",
    "import numpy as np\n",
    "merged_data['year'] = pd.to_datetime(merged_data['datadate']).dt.year\n",
    "\n",
    "\n",
    "# Example: Drop rows with missing values\n",
    "merged_data.dropna(subset=['intangible_ratio'], inplace=True)\n",
    "\n",
    "# Calculate percentiles\n",
    "intangible_ratio_q20 = np.percentile(merged_data['intangible_ratio'], 20)\n",
    "intangible_ratio_q40 = np.percentile(merged_data['intangible_ratio'], 40)\n",
    "intangible_ratio_q60 = np.percentile(merged_data['intangible_ratio'], 60)\n",
    "intangible_ratio_q80 = np.percentile(merged_data['intangible_ratio'], 80)\n",
    "\n",
    "# Add new columns to the data frame\n",
    "merged_data['intangible_ratio_q20'] = intangible_ratio_q20\n",
    "merged_data['intangible_ratio_q40'] = intangible_ratio_q40\n",
    "merged_data['intangible_ratio_q60'] = intangible_ratio_q60\n",
    "merged_data['intangible_ratio_q80'] = intangible_ratio_q80\n",
    "\n",
    "# Print or use the calculated percentiles as needed\n",
    "print(\"20th Percentile:\", intangible_ratio_q20)\n",
    "print(\"40th Percentile:\", intangible_ratio_q40)\n",
    "print(\"60th Percentile:\", intangible_ratio_q60)\n",
    "print(\"80th Percentile:\", intangible_ratio_q80)\n",
    "\n",
    "# Example: Drop rows with missing values\n",
    "merged_data.dropna(subset=['market_cap'], inplace=True)\n",
    "\n",
    "# Calculate percentiles\n",
    "market_cap_q20 = np.percentile(merged_data['market_cap'], 20)\n",
    "market_cap_q40 = np.percentile(merged_data['market_cap'], 40)\n",
    "market_cap_q60 = np.percentile(merged_data['market_cap'], 60)\n",
    "market_cap_q80 = np.percentile(merged_data['market_cap'], 80)\n",
    "\n",
    "# Add new columns to the data frame\n",
    "merged_data['market_cap_q20'] = market_cap_q20\n",
    "merged_data['market_cap_q40'] = market_cap_q40\n",
    "merged_data['market_cap_q60'] = market_cap_q60\n",
    "merged_data['market_cap_q80'] = market_cap_q80\n",
    "\n",
    "# Print or use the calculated percentiles as needed\n",
    "print(\"20th Percentile:\", market_cap_q20)\n",
    "print(\"40th Percentile:\", market_cap_q40)\n",
    "print(\"60th Percentile:\", market_cap_q60)\n",
    "print(\"80th Percentile:\", market_cap_q80)\n",
    "\n",
    "merged_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns for the new DataFrame\n",
    "selected_columns = ['year', 'market_cap_q20', 'market_cap_q40', 'market_cap_q60', 'market_cap_q80',\n",
    "                    'intangible_ratio_q20', 'intangible_ratio_q40', 'intangible_ratio_q60', 'intangible_ratio_q80']\n",
    "\n",
    "# Create a new DataFrame 'output' with selected columns\n",
    "quintiles_df = pd.DataFrame(merged_data[selected_columns])\n",
    "\n",
    "# Print the new DataFrame 'output'\n",
    "quintiles_df\n",
    "\n",
    "# Output the updated quintiles_df data frame\n",
    "quintiles_df.to_csv('step_2d.txt', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2(e)**  \n",
    "Load crsp_dataset_final.txt into a data frame. Merge this data frame with the data frame from **Step 2(d)**, making sure to do the following:\n",
    "- Each firm-month observation from year *T* in crsp_dataset_final.txt should be matched to quintile values of the intangibles ratio and market capitalization from year *T-1*.\n",
    "- Decide whether to keep only observations in both data frames OR observations in both data frames and also observations that are only in crsp_dataset_final.txt. When doing so, think carefully about any restrictions to the data that you applied in earlier substeps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load crsp_dataset_final.txt into a data frame\n",
    "crsp_data_final = pd.read_csv('D:/Quantitative--Trading/Data/crsp_data_final.txt', delimiter='\\t')\n",
    "\n",
    "crsp_data_final.rename(columns={'date': 'datadate'}, inplace=True)\n",
    "\n",
    "crsp_data_final['year'] = pd.to_datetime(crsp_data_final['datadate']).dt.year\n",
    "\n",
    "crsp_data_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(crsp_data_final, merged_data, on=['gvkey', 'datadate','year'])\n",
    "\n",
    "merged_data\n",
    "\n",
    "# Output the updated merged data frame\n",
    "merged_data.to_csv('step_2e.txt', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2(f)**    \n",
    "Sort all firm-month observations into one of 25 portfolios, based on the quintiles of the intangibles ratio and market capitalization. Create a new column in the data frame from **Step 2(e)** that contains a numeric identifier of the portfolio into which each firm-month observation is sorted. \n",
    "\n",
    "The portfolios should be defined as follows:\n",
    "\n",
    "Portfolio 1: Intangibles ratio and market capitalization both in the lowest quintile (i.e., below the 20$^{th}$ percentiles)      \n",
    "Portfolio 2: Intangibles ratio in the lowest quintile, market capitalization in the second-lowest quintile (i.e., between the 20$^{th}$ and 40$^{th}$ percentiles)     \n",
    "Portfolio 3: Intangibles ratio in the lowest quintile, market capitalization in the middle quintile (i.e., between the 40$^{th}$ and 60$^{th}$ percentiles)   \n",
    "...    \n",
    "\n",
    "Portfolio 6: Intangibles ratio in the second-lowest quintile, market capitalization in the lowest quintile    \n",
    "Portfolio 7: Intangibles ratio and market capitalization both in the second-lowest quintile    \n",
    "Portfolio 8: Intangibles ratio in the second-lowest quintile, market capitalization in the middle quintile        \n",
    "....    \n",
    "\n",
    "Portfolio 23: Intangibles ratio in the highest quintile, market capitalization in the middle quintile        \n",
    "Portfolio 24: Intangibles ratio in the highest quintile, market capitalization in the second-highest quintile    \n",
    "Portfolio 25: Intangibles ratio and market capitalization both in the highest quintile  (i.e., above the 80$^{th}$ percentiles)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate the quintiles for intangible_ratio and market_cap\n",
    "intangibles_quintiles = pd.qcut(merged_data['intangible_ratio'], q=5, labels=False)\n",
    "market_cap_quintiles = pd.qcut(merged_data['market_cap'], q=5, labels=False)\n",
    "\n",
    "# Create a new column for portfolio identifier\n",
    "merged_data['portfolio'] = intangibles_quintiles * 5 + market_cap_quintiles + 1\n",
    "\n",
    "merged_data\n",
    "\n",
    "# Output the updated merged data frame\n",
    "merged_data.to_csv('step_2f.txt', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2(g)**  \n",
    "For each portfolio created in **Step 2(f)**, calculate the equal-weighted average monthly stock return across all firms in the portfolio. Create a new data frame that has one observation per month and contains the average monthly returns for the 25 portfolios. This data frame should have 26 columns: The date and the 25 portfolio returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Step 2(g): Calculate equal-weighted average monthly stock return for each portfolio\n",
    "\n",
    "# Create a new DataFrame to store the results\n",
    "portfolio_returns_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each unique portfolio\n",
    "for portfolio_number in merged_data['portfolio'].unique():\n",
    "    # Select rows corresponding to the current portfolio\n",
    "    portfolio_data = merged_data[merged_data['portfolio'] == portfolio_number]\n",
    "\n",
    "    # Calculate equal-weighted average monthly stock return\n",
    "    portfolio_return = portfolio_data.groupby('datadate')['stock_return'].mean()\n",
    "\n",
    "    # Append the results to the new DataFrame\n",
    "    portfolio_returns_df = pd.concat([portfolio_returns_df, portfolio_return], axis=1, sort=False)\n",
    "\n",
    "# Rename the columns to reflect the portfolio numbers\n",
    "portfolio_returns_df.columns = [f'Portfolio {i}' for i in range(1, 26)]\n",
    "\n",
    "# Set 'datadate' column as the index\n",
    "portfolio_returns_df.index.name = 'date'\n",
    "portfolio_returns_df.reset_index(inplace=True)\n",
    "\n",
    "# Create a new index starting from 0\n",
    "portfolio_returns_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the resulting DataFrame with average monthly returns for each portfolio\n",
    "portfolio_returns_df['date'] = pd.to_datetime(portfolio_returns_df['date'], format='%Y%m%d')\n",
    "portfolio_returns_df.head()\n",
    "\n",
    "portfolio_returns_df\n",
    "\n",
    "# Output the updated portfolio_returns_df data frame\n",
    "portfolio_returns_df.to_csv('step_2g.txt', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2(h)**    \n",
    "For each of the 25 portfolios, calculate the average return over all months from January 1980 through December 1999. Enter these values into Panel A of Table 1 below. Next, calculate the portfolio's average return over all months from January 2000 through December 2019, and enter those values into Panel B of the table.\n",
    "\n",
    "Then, interpret the stock return patterns in each panel. In particular, is there evidence that larger values of the intangibles_ratio are associated with higher stock retuns? If yes, does this association change over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "portfolio_returns_df['date'] = pd.to_datetime(portfolio_returns_df['date'])\n",
    "\n",
    "# Extract year from the 'date' column\n",
    "portfolio_returns_df['year'] = portfolio_returns_df['date'].dt.year\n",
    "\n",
    "# Filter data for Panel A (January 1980 through December 1999)\n",
    "panel_a_data = portfolio_returns_df[(portfolio_returns_df['year'] >= 1980) & (portfolio_returns_df['year'] <= 1999)]\n",
    "\n",
    "# Filter data for Panel B (January 2000 through December 2019)\n",
    "panel_b_data = portfolio_returns_df[(portfolio_returns_df['year'] >= 2000) & (portfolio_returns_df['year'] <= 2019)]\n",
    "\n",
    "# Output the updated panel_a_data data frame\n",
    "panel_a_data.to_csv('step_2h_1.txt', index=False)\n",
    "\n",
    "# Output the updated panel_b_data data frame\n",
    "panel_b_data.to_csv('step_2h_2.txt', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Answer by double clicking this window and filling out the cells in the table below. In that table, [Portfolio N] refers to the average return of Portfolio N across all months in the period. Also, INT is an abbreviation for intangibles_ratio.\n",
    "    <br><br>\n",
    "\n",
    "**Table 1, Panel A. Average Portfolio Returns from 1980--1999**    \n",
    "    \n",
    "| Quintiles of Size \\ Quintiles of INT | 1 = Lowest INT         | 2            | 3             | 4            |5 = Highest INT            |\n",
    "|:-------------------------------------|:----------------------:|:------------:|:-------------:|:-----------:|:-------------------------:|\n",
    "|**1 = Lowest Size**                   |[Portfolio 1]           |[Portfolio 6] |[Portfolio 11]  |[Portfolio 16] | [Portfolio 21] |\n",
    "|**2**                                 |[Portfolio 2]           |[Portfolio 7] |[Portfolio 12]  |[Portfolio 17] | [Portfolio 22] |\n",
    "|**3**                                 |[Portfolio 3]           |[Portfolio 8] |[Portfolio 13]  |[Portfolio 18] | [Portfolio 23] |    \n",
    "|**4**                                 |[Portfolio 4]           |[Portfolio 9] |[Portfolio 14]  |[Portfolio 19] | [Portfolio 24] |\n",
    "|**5 = Highest Size**                  |[Portfolio 5]           |[Portfolio 10]|[Portfolio 15]  |[Portfolio 20] | [Portfolio 25] |    \n",
    "\n",
    "    \n",
    "**Table 1, Panel B. Average Portfolio Returns from 2000--2019**    \n",
    "\n",
    "| Quintiles of Size \\ Quintiles of INT | 1 = Lowest INT         | 2            | 3             | 4            |5 = Highest INT            |\n",
    "|:-------------------------------------|:----------------------:|:------------:|:-------------:|:-----------:|:-------------------------:|\n",
    "|**1 = Lowest Size**                   |[Portfolio 1]           |[Portfolio 6] |[Portfolio 11]  |[Portfolio 16] | [Portfolio 21] |\n",
    "|**2**                                 |[Portfolio 2]           |[Portfolio 7] |[Portfolio 12]  |[Portfolio 17] | [Portfolio 22] |\n",
    "|**3**                                 |[Portfolio 3]           |[Portfolio 8] |[Portfolio 13]  |[Portfolio 18] | [Portfolio 23] |    \n",
    "|**4**                                 |[Portfolio 4]           |[Portfolio 9] |[Portfolio 14]  |[Portfolio 19] | [Portfolio 24] |\n",
    "|**5 = Highest Size**                  |[Portfolio 5]           |[Portfolio 10]|[Portfolio 15]  |[Portfolio 20] | [Portfolio 25] | \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Click on this box and type your interpretation of the table. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2(i)**    \n",
    "Load factor_data.txt into a data frame. Merge this data frame with the data frame created in **Step 2(g)**. Convert all portfolio returns into percentages, and subtract the risk-free rate from each portfolio return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load factor_data.txt into a data frame\n",
    "factor_data = pd.read_csv('D:/Quantitative--Trading/Data/factor_data-2.txt', delimiter='\\t')\n",
    "\n",
    "factor_data['date'] = pd.to_datetime(factor_data['date'], format='%Y%m')\n",
    "\n",
    "\n",
    "# Convert the 'date' column in 'portfolio_returns_df' to the same type as in 'factor_data'\n",
    "portfolio_returns_df['date'] = pd.to_datetime(portfolio_returns_df['date'], format='%Y%m')\n",
    "\n",
    "# Merge factor_data with portfolio_returns_df\n",
    "merged_data_with_factors = pd.merge(portfolio_returns_df, factor_data, on='date')\n",
    "\n",
    "# Convert portfolio returns into percentages\n",
    "for portfolio in range(1, 26):\n",
    "    portfolio_name = f'Portfolio {portfolio}'\n",
    "    merged_data_with_factors[portfolio_name] *= 100  # Convert to percentage\n",
    "\n",
    "# Subtract rf_rate from each portfolio return\n",
    "for portfolio in range(1, 26):\n",
    "    portfolio_name = f'Portfolio {portfolio}'\n",
    "    merged_data_with_factors[portfolio_name] -= merged_data_with_factors['rf_rate']\n",
    "\n",
    "merged_data_with_factors\n",
    "\n",
    "# Output the updated merged_data_with_factors data frame\n",
    "merged_data_with_factors.to_csv('step_2i.txt', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2(j)**    \n",
    "For each of the 25 portfolios, regress the monthly returns (net of risk-free rate) on the three factors, over all months from January 1980 through December 1999. For each regression, store the estimated intercept (i.e., the alpha) and the intercept's p-value in a data frame. Next, re-estimate the same regressions over all months from January 2000 through December 2019, and store those alphas and p-values in the same data frame. This data frame should contain five columns: The portfolio number, the portfolio's alpha over the 1980--1999 period, the p-value of the portfolio's intercept over the 1980--1999 period, the portfolio's alpha over the 2000-2019 period, and the p-value of the portfolio's intercept over the 2000--2019 period. Print this data frame to an output file, and upload this file as part of your final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Assuming you have the DataFrame named 'merged_data_with_factors'\n",
    "# with the specified columns in your dataset\n",
    "\n",
    "# Create a DataFrame to store regression results\n",
    "regression_results_df = pd.DataFrame(columns=['Portfolio', 'Alpha_1980_1999', 'P_Value_1980_1999', 'Alpha_2000_2019', 'P_Value_2000_2019'])\n",
    "\n",
    "# Iterate over each portfolio\n",
    "for portfolio in range(1, 26):\n",
    "    portfolio_name = f'Portfolio {portfolio}'\n",
    "\n",
    "    # Extract data for the current portfolio\n",
    "    portfolio_data = merged_data_with_factors[['date', portfolio_name, 'rf_rate', 'SMB', 'HML']]\n",
    "\n",
    "    # Separate data for the two time periods\n",
    "    data_1980_1999 = portfolio_data[(portfolio_data['date'] >= start_date_1980) & (portfolio_data['date'] <= end_date_1999)]\n",
    "    data_2000_2019 = portfolio_data[(portfolio_data['date'] >= start_date_2000) & (portfolio_data['date'] <= end_date_2019)]\n",
    "\n",
    "    # Check if there is sufficient data for regression\n",
    "    if len(data_1980_1999) > 0 and len(data_2000_2019) > 0:\n",
    "        # Perform regression for 1980-1999\n",
    "        X_1980_1999 = sm.add_constant(data_1980_1999[['SMB', 'HML']])\n",
    "        y_1980_1999 = data_1980_1999[portfolio_name] - data_1980_1999['rf_rate']\n",
    "        model_1980_1999 = sm.OLS(y_1980_1999, X_1980_1999).fit()\n",
    "        alpha_1980_1999 = model_1980_1999.params['const']\n",
    "        p_value_1980_1999 = model_1980_1999.pvalues['const']\n",
    "\n",
    "        # Perform regression for 2000-2019\n",
    "        X_2000_2019 = sm.add_constant(data_2000_2019[['SMB', 'HML']])\n",
    "        y_2000_2019 = data_2000_2019[portfolio_name] - data_2000_2019['rf_rate']\n",
    "        model_2000_2019 = sm.OLS(y_2000_2019, X_2000_2019).fit()\n",
    "        alpha_2000_2019 = model_2000_2019.params['const']\n",
    "        p_value_2000_2019 = model_2000_2019.pvalues['const']\n",
    "\n",
    "        # Append results to the DataFrame\n",
    "        regression_results_df = regression_results_df.append({\n",
    "            'Portfolio': portfolio,\n",
    "            'Alpha_1980_1999': alpha_1980_1999,\n",
    "            'P_Value_1980_1999': p_value_1980_1999,\n",
    "            'Alpha_2000_2019': alpha_2000_2019,\n",
    "            'P_Value_2000_2019': p_value_2000_2019\n",
    "        }, ignore_index=True)\n",
    "\n",
    "# Print the DataFrame with regression results\n",
    "print(regression_results_df)\n",
    "\n",
    "# Save the DataFrame to an output file (e.g., CSV)\n",
    "regression_results_df.to_csv('regression_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2(k)**    \n",
    "Enter the alphas estimated over the 1980--1999 period into Panel A of Table 2 below, and enter the alphas estimated over the 2000--2019 period into Panel B. Also, place an asterix * next to any alpha that has a p-value less than 0.05.\n",
    "\n",
    "Then, interpret the alphas in each panel. How many portfolios outperform the expected return based on the Fama-French 3-factor model? Is there any correlation between portfolio outperformance and the level of the intangibes ratio? If yes, does this correlation change over time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Answer by double clicking this window and filling out the cells in the table below. In that table, [Portfolio N] refers to the regression intercept for Portfolio N, estimated across all months in the period. Also, INT is an abbreviation for intangibles_ratio.\n",
    "    <br><br>\n",
    "\n",
    "**Table 2, Panel A. Portfolio Alphas from 1980--1999**    \n",
    "    \n",
    "| Quintiles of Size \\ Quintiles of INT | 1 = Lowest INT         | 2            | 3             | 4            |5 = Highest INT            |\n",
    "|:-------------------------------------|:----------------------:|:------------:|:-------------:|:-----------:|:-------------------------:|\n",
    "|**1 = Lowest Size**                   |[Portfolio 1]           |[Portfolio 6] |[Portfolio 11]  |[Portfolio 16] | [Portfolio 21] |\n",
    "|**2**                                 |[Portfolio 2]           |[Portfolio 7] |[Portfolio 12]  |[Portfolio 17] | [Portfolio 22] |\n",
    "|**3**                                 |[Portfolio 3]           |[Portfolio 8] |[Portfolio 13]  |[Portfolio 18] | [Portfolio 23] |    \n",
    "|**4**                                 |[Portfolio 4]           |[Portfolio 9] |[Portfolio 14]  |[Portfolio 19] | [Portfolio 24] |\n",
    "|**5 = Highest Size**                  |[Portfolio 5]           |[Portfolio 10]|[Portfolio 15]  |[Portfolio 20] | [Portfolio 25] |    \n",
    "\n",
    "    \n",
    "**Table 2, Panel B. Portfolio Alphas from 2000--2019**    \n",
    "\n",
    "| Quintiles of Size \\ Quintiles of INT | 1 = Lowest INT         | 2            | 3             | 4            |5 = Highest INT            |\n",
    "|:-------------------------------------|:----------------------:|:------------:|:-------------:|:-----------:|:-------------------------:|\n",
    "|**1 = Lowest Size**                   |[Portfolio 1]           |[Portfolio 6] |[Portfolio 11]  |[Portfolio 16] | [Portfolio 21] |\n",
    "|**2**                                 |[Portfolio 2]           |[Portfolio 7] |[Portfolio 12]  |[Portfolio 17] | [Portfolio 22] |\n",
    "|**3**                                 |[Portfolio 3]           |[Portfolio 8] |[Portfolio 13]  |[Portfolio 18] | [Portfolio 23] |    \n",
    "|**4**                                 |[Portfolio 4]           |[Portfolio 9] |[Portfolio 14]  |[Portfolio 19] | [Portfolio 24] |\n",
    "|**5 = Highest Size**                  |[Portfolio 5]           |[Portfolio 10]|[Portfolio 15]  |[Portfolio 20] | [Portfolio 25] | \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Click on this box and type your interpretation of the table. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge Exercises**    \n",
    "The below exercises are intended for students who wish to earn a grade above 9,0 on the final assignment (i.e., to earn an A+). They should be possible to complete with the computing knowledge and syntax taught in the course. However, the exercises are more challenging and can take significantly more time.\n",
    "\n",
    "Students who do not attempt the challenge exercises can receive a grade of up to 9,0 on their final assignment. Students who make a serious attempt at the challenge exercises will receive a bonus of up to 1,0 points on the final assignment. The bonus is awarded even to students who do not correctly complete all of the main steps of the final assignment. However, no bonus is awarded to students who do not attempt at least half of the main steps. (The bonus is calculated one time over all of the challenge exercises in this assignment.)\n",
    "\n",
    "The exercises for **Step 2** are:\n",
    "- In **Step 2(g)**, construct weighted-average portfolio returns instead of equal-weighted returns. For weights, use each firm's total_assets.\n",
    "- Some researchers argue that the *HML* factor has become less important for firms' expected stock returns over time, due to the rising usage of intangible assets. Read briefly about changes to the *HML* factor over time and summarize the argument. Then, repeat **Steps 2(f)** through **2(h)** by sorting portfolio using intangibles_ratio and the book equity/market equity measure (instead of market capilization itself). In **Step 2(h)**, do the return patterns for portfolios with high versus low values of intangibles_ratio change significantly?\n",
    "- Read only the introduction of the paper \"A five-factor asset pricing model\" by Eugene Fama and Ken French, published in 2015 in the *Journal of Financial Economics* (Volume 116). This paper explains the theory behind the 5-Factor model, which is a proposed update to the 3-Factor model from 1993. Then, repeat **Steps 2(i)** through **2(k)** using a 5-factor model. You can obtain data on the 5 factors here: https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Click on this box and type your answers. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENTER YOUR CODE IN THIS BOX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Preparing Data for a Pairs Trading Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** One simple example of a convergence arbitrage strategy is Pairs Trading. For this strategy, a fund identifies two or more securities that are closely related to each other. Theoretically, the securities' prices should co-move in the same direction over time, due to their similarities. The securities *might not* have exactly the same price, but the difference in their prices should be relatively stable over time. The strategy works by (i) identifying the securities; (ii) measuring the difference in their prices during \"normal\" trading periods; (iii) identifying any points in time where the difference in prices significantly deviates from the usual amount; and (iv) implementing a zero-net-cost arbitrage trade that earns a profit if the difference in prices shrinks back to the usual amount.  \n",
    "\n",
    "In this step, you will prepare data for implementing a Pairs Trading strategy using corporate bonds. Specifically, you will combine several datasets that contain bond-level and firm-level information, and then you will identify groups of similar bonds that could be used in Pairs Trading. The purpose is to understand the underlying data work that must be completed, before a precise trading strategy can be designed.\n",
    "\n",
    "**Note:** For this step you can use Pandas, but are not required to.\n",
    "\n",
    "**Getting Started**: To complete this part, you will need these files:\n",
    "- bond_descriptions.txt, which contains information on individual bonds issued by U.S. firms in the TRACE dataset which are at traded at least occassionally. \n",
    "   - In this file, unsecured_indicator identifies whether or not the bond is backed by some collateral, which investors can receive and re-sell in case of bankruptcy.\n",
    "   - junior_indicator identifies whether or not the bond is junior in the priority order. In bankruptcy, these bonds are paid out only if sufficient cash is left after paying out senior bonds.\n",
    "   - convertible_indicator identifies whether or not the bond can be converted to equity, at the choice of the firm.\n",
    "- trace_linkfile.txt, which contains indicators matching individual bonds to the Compustat GVKEY of the firm that issued them\n",
    "- bond_rating.txt, which contains credit ratings of firms in the Compustat dataset\n",
    "\n",
    "**To Complete:**\n",
    "- Fill in all code boxes\n",
    "- Fill in all answer boxes.\n",
    "- Submit the output file created in **Step 3(f)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3(a)**     \n",
    "Open bond_descriptions.txt with any program and examine the dataset. What does each observation represent, and what variable(s) uniquely identify it? How much variation exists among the bonds in terms of secured status, priority order, or convertibility? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Click on this box and type your answers. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3(b)**     \n",
    "Restrict bond_descriptions.txt to only those bonds that are issued on January 1, 2005 or later. Next, combine bond_descriptions.txt with the identifying variables in trace_linkfile.txt. Decide whether to keep only observations that are in both files OR observations that are in both files plus observations that are only in trace_linkfile.txt. Briefly explain your choice in a comment in the code box below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENTER YOUR CODE IN THIS BOX\n",
    "import pandas as pd\n",
    "#load bond_descriptions file\n",
    "bond_descriptions = pd.read_csv(\"C:/Users/yachi/final/Data/bond_descriptions.txt\", delimiter='\\t')\n",
    "#load trace_linkfile file\n",
    "trace_linkfile = pd.read_csv(\"C:/Users/yachi/final/Data//trace_linkfile.txt\", delimiter='\\t')\n",
    "#change the format of date\n",
    "bond_descriptions['issue_date'] = pd.to_datetime(bond_descriptions['issue_date'], format='%Y%m%d')\n",
    "#filter issue_Date from 2005-01-01\n",
    "filtered_bonds = bond_descriptions[bond_descriptions['issue_date'] >= '2005-01-01']\n",
    "#merge bond_descriptions and trace_linkfile in total_data\n",
    "total_data = pd.merge(filtered_bonds, trace_linkfile, how='inner', on=['bond_sym_id'])\n",
    "#output is updated\n",
    "total_data.to_csv('step3b.txt', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3(c)**     \n",
    "Next, add the firm's credit rating at the time of the bond's issuance to the modified dataset. First, open bond_ratings.txt with any program and examine the dataset. What variable(s) uniquely identify each observation? Explain briefly how you can merge the credit ratings with the bond's issuance date. Then, write the code to execute the merge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Click on this box and type your explanation of the merge process. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import data called bond_ratings\n",
    "bond_ratings = pd.read_csv(\"C:/Users/yachi/final/Data/bond_ratings.txt\", delimiter='\\t')\n",
    "#change the date format\n",
    "bond_ratings['datadate'] = pd.to_datetime(bond_ratings['datadate'], format='%Y%m%d')\n",
    "#merge total_file and bond_ratings into merged_data_ratings\n",
    "merged_data_ratings = pd.merge(total_data, bond_ratings, on=['gvkey'], how='left')\n",
    "#output is updated\n",
    "merged_data_ratings.to_csv('step3c.txt', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3(d)**     \n",
    "Now, begin your search for bonds that are similar to each other. Start by grouping bonds that are issued in the same calendar month and by firms in the same industry. For example, one group could contain all bonds issued in March 2005 by firms in the Insurance industry. Then, write a script that can count the number of unique bonds in each group. Are there some groups with at least three bonds issued in the same month and industry? Are there any groups with at least 10 bonds? \n",
    "\n",
    "**Hint:** If you are using Pandas, then this step can be solved with a command that we did not cover in class. This documentation may help: https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.count.html#pandas.core.groupby.DataFrameGroupBy.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminate date from issue_date, it is no longer usable\n",
    "merged_data_ratings['issue_month'] = merged_data_ratings['issue_date'].dt.to_period('M')\n",
    "#make new data by grouping\n",
    "grouped_data = merged_data_ratings.groupby(['issue_month', 'industry'])['bond_sym_id'].nunique().reset_index()\n",
    "#atleast 3 bonds in grouped data\n",
    "three_bonds = grouped_data[grouped_data['bond_sym_id'] >= 3]\n",
    "#atleast 10 bonds in grouped data\n",
    "ten_bonds = grouped_data[grouped_data['bond_sym_id'] >= 10]\n",
    "#output is updated\n",
    "three_bonds.to_csv('step3d-1.txt', index=False)\n",
    "#output is updated\n",
    "ten_bonds.to_csv('step3d-2.txt', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3(e)**     \n",
    "Consider a \"pairs trading\" strategy that involves bonds issued in the same calendar month by firms in the same industry. Such a strategy is only effective if the bonds involved in the trade are highly similar to each other, in terms of characteristics that matter for bond pricing. Explain the advantage of using a pairs trading strategy based on month and industry, over a strategy that compares all bonds issued in the same calendar month (without considering industry). Also, state one or two other factors that are unaccounted for in this strategy, and explain how they might affect bond pricing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Click on this box and type your answers. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3(f)**     \n",
    "Consider a \"pairs trading\" strategy that involves bonds issued in the same calendar month by firms in the same industry AND that also share one other similarity that matters for bond pricing. Briefy state the similarity that you chose in a comment in the code box below. Then, repeat **Step 3(d)** for the new pairs trading strategy. \n",
    "\n",
    "Further, create an output file that includes at least two columns: An identifier for each group, and the number of bonds in the group. Upload this file as part of your final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make similar format of dates \n",
    "merged_data_ratings['issue_month'] = merged_data_ratings['issue_date'].dt.to_period('M')\n",
    "#make new column by grouping and average named similarity\n",
    "merged_data_ratings['similarity'] = merged_data_ratings.groupby(['issue_month', 'industry'])['coupon_rate'].transform('mean')\n",
    "#make new dataframe for finding bond similarity\n",
    "grouped_data_pairs = merged_data_ratings.groupby(['issue_month', 'industry', 'similarity'])['bond_sym_id'].nunique().reset_index()\n",
    "#atleast more than 3 pairs in a group\n",
    "three_bonds_pairs = grouped_data_pairs[grouped_data_pairs['bond_sym_id'] >= 3]\n",
    "#atleast more than 10 pairs in a group\n",
    "ten_bonds_pairs = grouped_data_pairs[grouped_data_pairs['bond_sym_id'] >= 10]\n",
    "#update output file\n",
    "grouped_data_pairs.to_csv('step3f.txt', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3(g)**     \n",
    "In **Steps 3(d)** and **3(f)** above, you counted the number of unique bonds per group. It may also be useful to determine whether the bonds in each group are all issued by the same firm, or whether the group contains bonds issued by multiple firms. Write a script that can determine this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3(h)**     \n",
    "Suppose you have a dataset of all bond trades, which includes the bond_sym_id identifier, the time and date of the trade, and the price and yield at which each bond is traded. Consider the \"pairs trading\" strategy devised in **Step 3(f)**, and suppose that you use a group that contains two bonds. (You don't need to consider whether they are issued by the same firm.) Provide a written set of instructions for how you could determine whether the two bonds' prices deviate from each other, in such a way that an arbitrage trade may be possible. Make the instructions highly detailed, so that they could be easily converted into a Python script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Click on this box and type your answers. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge Exercises**    \n",
    "The below exercises are intended for students who wish to earn a grade above 9,0 on the final assignment (i.e., to earn an A+). They should be possible to complete with the computing knowledge and syntax taught in the course. However, the exercises are more challenging and can take significantly more time.\n",
    "\n",
    "Students who do not attempt the challenge exercises can receive a grade of up to 9,0 on their final assignment. Students who make a serious attempt at the challenge exercises will receive a bonus of up to 1,0 points on the final assignment. The bonus is awarded even to students who do not correctly complete all of the main steps of the final assignment. However, no bonus is awarded to students who do not attempt at least half of the main steps. (The bonus is calculated one time over all of the challenge exercises in this assignment.)\n",
    "\n",
    "The exercises for **Step 3** are:\n",
    "- Repeat **Step 3(d)**, except this time create groups of bonds that are issued within 15 calendar days of each other and by firms in the same industry. For example, one group could contain all bonds issued between February 20, 2005 and March 7, 2005 by firms in the Insurance industry.\n",
    "- Write a script to implement the proposed strategy from **Step 3(h)**. Contact t.ladika@uva.nl by December 15 to request the full dataset on bond trades. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENTER YOUR CODE IN THIS BOX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
